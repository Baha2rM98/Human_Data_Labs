{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural machine translation through sequence-to-sequence model with attention - Part B\n",
    "\n",
    "Welcome to the seven HDA laboratory! In this notebook, you will use the building blocks you implemented yesterday to build the sequence-to-sequence neural machine translator. You will implement a customized training pipeline and you will finally obtain a network able to translate words from Italian to English!\n",
    "\n",
    "In this notebook, sections 1, 2, 3 and 4 are already completed with the solutions from Laboratory 6. If you don't remember the structure go through them, otherwise you can directly start from section 5.\n",
    "\n",
    "This notebook is a revisitation of the one proposed by [TensorFlow](https://www.tensorflow.org/text/tutorials/nmt_with_attention). \n",
    "If you want to experiment more, once completed this notebook you can also go through the notebook about [Transformers](https://www.tensorflow.org/text/tutorials/transformer). Prof. Rossi will go through the transformer architecture during the January lessons.  \n",
    "\n",
    "**In this assignment, you will:**\n",
    "- Put together the building blocks from Laboratory 6 to create a sequence-to-sequence neural machine translator\n",
    "\n",
    "**NOTE**\n",
    "For this lab, we will provide you also with the *.py* files so that you can run the code with a Python IDE. We suggest using a Python IDE instead of Jupyter Notebooks for developing the code for your final project as the debugging would be easier (you can find some suggestions in the how-to document in Moodle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dataset_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "Download the dataset from [here](http://www.manythings.org/anki/) and unzip it in the same folder of the notebook. In this notebook the ita-eng one is chosen but you can select the one you prefer and change the first line below.\n",
    "The `buff_size` variable is to select a small portion of the dataset for reducing the complexity of the training, you can increase it to obtain a better translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quello è un tavolo.\n",
      "That is a table.\n"
     ]
    }
   ],
   "source": [
    "path_to_file = pathlib.Path('./ita-eng/ita.txt')\n",
    "buff_size = 30000\n",
    "inp, targ = load_data(path_to_file, buff_size)\n",
    "print(inp[-2])\n",
    "print(targ[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(buff_size * 0.8)\n",
    "\n",
    "inp_train = inp[:train_len]\n",
    "targ_train = targ[:train_len]\n",
    "\n",
    "inp_test = inp[train_len:]\n",
    "targ_test = targ[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training and test datasets. Use the function [tf.data.Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) to create the dataset with ``input`` and ``target`` pairs. Hence, cache the dataset, and apply the shuffling and the baching operations (see Lab 5 for reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:17:40.813943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:40.889800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:40.891515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:40.895000: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-21 11:17:40.898961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:40.900626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:40.902283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:41.571792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:41.572126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:41.572413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-21 11:17:41.572689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6820 MB memory:  -> device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "### START CODE HERE ###\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((inp_train, targ_train))\n",
    "cache_file_train = 'dataset_cache_train'\n",
    "dataset_train = dataset_train.cache(cache_file_train)\n",
    "dataset_train = dataset_train.shuffle(buff_size)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "### END CODE HERE ###\n",
    "\n",
    "### START CODE HERE ###\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((inp_test, targ_test))\n",
    "cache_file_test = 'dataset_cache_test'\n",
    "dataset_test = dataset_test.cache(cache_file_test)\n",
    "dataset_test = dataset_test.shuffle(buff_size)\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [tf.keras.layers.TextVectorization layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) that you will use in the following to map the text into integer sequences. We use the custom `tf_lower_and_split_punct` in `dataset_utils.py` for the standardization (e.g., remove accents, and put all letters in lowercase...). This operation is needed because we want to use a vocabulary with a limited size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
    "                                                         max_tokens=max_vocab_size)\n",
    "input_text_processor.adapt(inp_train)\n",
    "\n",
    "output_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
    "                                                          max_tokens=max_vocab_size)\n",
    "output_text_processor.adapt(targ_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to have an example of how the preprocessing is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! You've almost done, this is the sixth HDA lab. Oggi è il sesto laboratorio del corso HDA!\n",
      "[START] hi !  you ' ve almost done ,  this is the sixth hda lab .  oggi e il sesto laboratorio del corso hda ! [END]\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('Hi! You\\'ve almost done, this is the sixth HDA lab. Oggi è il sesto laboratorio del corso HDA!')\n",
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Encoder\n",
    "Let's start to create the seq2seq model. \n",
    "<center><img src=\"images/seq2seq.jpg\" style=\"width:50%\"></center>\n",
    "<caption><center> The seq2seq model with attention.<br> </center></caption>\n",
    "\n",
    "The first block we need to code is the encoder. This block processes the input sequence and encodes it into a code of fixed size.\n",
    "\n",
    "To create the encoder we define a [custom tf.keras.Layer](https://www.tensorflow.org/guide/keras/custom_layers_and_models) by subclassing the tf.keras.Layer. We will instantiate all the sub-layers that we need for this new layer and combine them to create the new layer. \n",
    "Here in the ``init`` method we instantiate all the sub-layers needed to define the encoder. Each layer will be an attribut of the class, i.e., it will be referred to as ``self.attribute_name`` within the class.  \n",
    "Next, the `call` method creates the layer by combining the sub-layers instantiated above: first the input is forwarded through the embedding sub.layer and the output of the embedding is forwarded through the RNN with GRU sub-layer. \n",
    "    \n",
    "Specifically, through the ``call`` method, the encoder:\n",
    "1. takes a list of token indices\n",
    "2. converts each token into an embedding vector using a [layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
    "3. processes the embeddings sequentially through a [tf.keras.layers.GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) and obtain the output state (`return_state`=True) and the entire sequence of hidden states resulting from the processing of the entire sequence (`return_sequences`=True), use `glorot_uniform` for the initializer\n",
    "4. returns the processed sequence that will be passed to the attention layer and the internal state stat will serve to initialize the decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        \"\"\"\n",
    "        This class creates a new custom tf.keras.Layer. This is done by instantiating some attributes and combining them.\n",
    "        For more info see: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "        Here we instantiate all the layers that we need for this model. \n",
    "        Next, the `call` method create the layer by combining the ones instantiated here.\n",
    "        :param input_vocab_size: the size of the vocabulary\n",
    "        :param embedding_dim: fixed size for the embeddings\n",
    "        :param enc_units: number of neurons for the encoding\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # Instantiate an embedding layer to convert tokens into vectors\n",
    "        # see https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "        ### START CODE HERE ###\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Instantiate a GRU RNN layer to process those vectors sequentially\n",
    "        ### START CODE HERE ###\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        \"\"\"\n",
    "        This function create the custom tf.keras.Layer structure by combining the blocks defined in the __init__\n",
    "        :param tokens: the input tokens\n",
    "        :param state: the info state for the GRU RNN (if present)\n",
    "        :return: the outout and the state of the GRU RNN\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        vectors = self.embedding(tokens)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        ### END CODE HERE ###\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the correct functioning of the encoder by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Tutto bene?' b'Lei lo vuole?' b'Tom non \\xc3\\xa8 andato.'\n",
      " b\"L'ho visto saltare.\" b'\\xc3\\x88 intatta.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'You all right?' b'Do you want it?' b\"Tom didn't go.\" b'I saw him jump.'\n",
      " b\"It's undamaged.\"], shape=(5,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:17:42.924545: W tensorflow/core/kernels/data/cache_dataset_ops.cc:233] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 10)\n",
      "Encoder output, shape (batch, s, units): (64, 10, 1024)\n",
      "Encoder state, shape (batch, units): (64, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:17:43.624176: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "for example_input_batch, example_target_batch in dataset_train.take(1):\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break\n",
    "\n",
    "# Convert the input text to tokens using the input_text_processor layer \n",
    "# that you created above as an instance of tf.keras.layers.TextVectorization\n",
    "### START CODE HERE ###\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Instantiate an object of the Encoder class\n",
    "### START CODE HERE ###\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Encode the input sequence using the Encoder object you have just instantiated\n",
    "### START CODE HERE ###\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Attention layer\n",
    "The decoder uses attention to selectively focus on parts of the input sequence. The attention takes a sequence of vectors as input for each example and returns an attention vector for each example.\n",
    "We use the attention layer [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention) that follows the [Bahdanau-style attention](https://arxiv.org/pdf/1409.0473.pdf). \n",
    "The query is the decoder state ($\\boldsymbol{h}_t$) attending to the sequence while the value is the encoder output ($\\bar{\\boldsymbol{h}}_s$) being attended to.\n",
    "The attention weights and the context vector are computed as follows.\n",
    "\n",
    "$$\\large{\\rm{score}(\\boldsymbol{h}_t, \\boldsymbol{\\bar{h}}_s) = \\rm{tanh}(\\boldsymbol{W_1}\\boldsymbol{h}_t + \\boldsymbol{W_2}\\boldsymbol{\\bar{h}}_s)}$$\n",
    "\n",
    "<center><img src=\"images/attention_equation_1.jpeg\" style=\"width:50%\"></center>\n",
    "<center><img src=\"images/attention_equation_2.jpeg\" style=\"width:50%\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"\n",
    "        This class creates a new custom tf.keras.Layer. This is done by instantiating some attributes and combining them.\n",
    "        For more info see: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "        Here we instantiate all the layers that we need for this model. \n",
    "        Next, the `call` method create the layer by combining the ones instantiated here.\n",
    "        :param units: number of units for the Dense layer\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # Instantiate two Dense layers for the query (decoder state) and for the value (encoder output)\n",
    "        # with 'units' output neurons, use use_bias=False\n",
    "        ### START CODE HERE (2 lines) ### \n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Instantiate a AdditiveAttention layer\n",
    "        ### START CODE HERE ###\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "\n",
    "        # Pass the query (the decoder state attending to the sequence) through the first dense layer\n",
    "        ### START CODE HERE ###\n",
    "        w1_query = self.W1(query)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Pass the value (the sequence of encoder outputs being attended to) through the second dense layer\n",
    "        ### START CODE HERE ###\n",
    "        w2_key = self.W2(value)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        context_vector, attention_weights = self.attention(inputs=[w1_query, value, w2_key],\n",
    "                                                           mask=[query_mask, value_mask],\n",
    "                                                           return_attention_scores=True)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the correct functioning of the attention layer by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch_size, query_seq_length, units): (64, 2, 1024)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "attention_layer = BahdanauAttention(units)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Later, the decoder will generate this attention query\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "# Attend to the encoded tokens, the mask is used to exclude the padding\n",
    "context_vector, attention_weights = attention_layer(query=example_attention_query,\n",
    "                                                    va,\n",
    "                                                    mask=(example_tokens != 0))\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units): {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19072419583797456, 0.21191577315330506)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_slice = attention_weights[0, 0].numpy()\n",
    "attention_slice = attention_slice[attention_slice != 0]\n",
    "\n",
    "plt.suptitle('Attention weights for one sequence')\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(range(len(attention_slice)), attention_slice)\n",
    "plt.xlabel('Attention weights')\n",
    "\n",
    "# zoom in\n",
    "top = max(plt.ylim())\n",
    "zoom = 0.85*top\n",
    "plt.ylim([0.90*top, top])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Decoder\n",
    "The decoder generates the prediction for the next output token starting from the encoder output\n",
    "\n",
    "1. it uses an RNN to keep track of what it has predicted previously\n",
    "2. the RNN output is used as a query for the attention layer to attend the encoder's output and produce the context vector\n",
    "3. the context vector is combined with the RNN output to generate the attention vector using the equation\n",
    "\n",
    "$$\\large{\\boldsymbol{a}_t = \\rm{tanh}(\\boldsymbol{W}_c[\\boldsymbol{c}_t; \\boldsymbol{h}_t])}~~~~~\\rm{[attention~vector]}$$\n",
    "\n",
    "4. finally, it generates logit predictions for the next token based on the attention vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        \"\"\"\n",
    "        This class creates a new custom tf.keras.Layer. This is done by instantiating some variables and combining them.\n",
    "        For more info see: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "        Here we instantiate all the layers that we need for this model. \n",
    "        Next, the `call` method create the layer by combining the ones instantiated here.\n",
    "        :param output_vocab_size:\n",
    "        :param embedding_dim:\n",
    "        :param dec_units:\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Instantiate an embedding layer to convet token IDs to vectors\n",
    "        ### START CODE HERE ###\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size, embedding_dim)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Instantiate an RNN to keep track of what's wii be generated from time to time by the decoder\n",
    "        ### START CODE HERE ###\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Instantiate a BahdanauAttention layer that will obtain as input the RNN output, i.e., the RNN output\n",
    "        # will be the query to the attention layer over the encoder's output to produce the context vector\n",
    "        ### START CODE HERE ###\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Instantiate a Dense layer to combine the RNN output and the context vector to generate the attention vector\n",
    "        ### START CODE HERE ###\n",
    "        self.Wc = tf.keras.layers.Dense(self.dec_units, activation=tf.math.tanh, use_bias=False)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Instantiate a fully connected layer to produce the logits for each output token based on the attention vector\n",
    "        ### START CODE HERE ###\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def call(self, new_tokens, enc_output, mask, state=None):\n",
    "        # Step 1. Lookup the embeddings\n",
    "        ### START CODE HERE ###\n",
    "        vectors = self.embedding(new_tokens)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Step 2. Process one step with the RNN\n",
    "        ### START CODE HERE ###\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Step 3. Use the RNN output as the query for the attention over the encoder output\n",
    "        ### START CODE HERE ###\n",
    "        context_vector, attention_weights = self.attention(query=rnn_output, value=enc_output, mask=mask)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Step 4. Concatenate the context_vector and rnn_output -- shape: (batch t, value_units + query_units)\n",
    "        ### START CODE HERE ###\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Step 4. Obtain the attention vector: attention_vector = tanh(Wc@context_and_rnn_output)\n",
    "        ### START CODE HERE ###\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Step 5. Generate logit predictions through the final dense layer\n",
    "        ### START CODE HERE ###\n",
    "        logits = self.fc(attention_vector)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return logits, attention_weights, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the correct functioning of the decoder layer by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (batch_size, t, output_vocab_size) (64, 1, 2619)\n",
      "state shape: (batch_size, dec_units) (64, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['bus'],\n",
       "       ['fatal'],\n",
       "       ['forgave'],\n",
       "       ['dug'],\n",
       "       ['crashed']], dtype='<U12')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "### END CODE HERE ###\n",
    "    \n",
    "# Convert the target sequence using the output_text_processor object you instantiated above\n",
    "### START CODE HERE ###\n",
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Collect the \"[START]\" tokens\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n",
    "\n",
    "# Run the decoder\n",
    "### START CODE HERE ###\n",
    "dec_logits, dec_attention, dec_state = decoder(first_token, enc_output=example_enc_output, mask=(example_tokens != 0), state = example_enc_state)\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')\n",
    "\n",
    "sampled_token = tf.random.categorical(dec_logits[:, 0, :], num_samples=1)\n",
    "vocab = np.array(output_text_processor.get_vocabulary())\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Loss function\n",
    "Here we create a custom loss function that will be used during training for computing the gradients to be backpropagated and allow updating the weights. The construction is similar to the one seen for the custom layers above,\n",
    "[see here for more info](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Here we create a custom loss function. The construction is similar to the one seen for the custom layers above.\n",
    "        See https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses\n",
    "        \"\"\"\n",
    "        super(MaskedLoss, self).__init__()\n",
    "        \n",
    "        self.name = 'masked_loss'\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # Use the correct boolean for the from_logits argument\n",
    "        # Use reduction='none' as you will compute the reduced sum in the __call__ method\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # Calculate the loss for each item in the batch\n",
    "        ### START CODE HERE ###\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Mask off the losses on padding\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total (use tf.reduce_sum)\n",
    "        ### START CODE HERE ###\n",
    "        return tf.reduce_sum(loss)\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Implement the training model\n",
    "Now we create a new custom model by [subclassing the tf.keras.Model class](https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class). The process is similar to the one followed to subclass the tf.keras.Layers and tf.keras.losses.Loss classes above. \n",
    "\n",
    "Specifically, the `train_step` method overwrites the built-in `train_step` method of the tf.keras.Model class. \n",
    "The `train_step` method is the function called by `fit()` for every batch of data. By overwriting it, we can customize the operations performed during the `fit()` call for our custom TranslatorModelTrain model.\n",
    "We can then call `fit()` as usual -- and it will be running our own learning algorithm.\n",
    "For additional info see [this reference](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). Do you remember what you see in Laboratory 2 about the two approaches to train a neural network, i.e., custom training and training with built-in Tensorflow functions? Here you are going to combine the two approaches by still using the built-in-functions while changing their behaviour. (Hint: to complete the following portion of the code it will be useful to take a look at what you implemented in Lab 2 on Section 2.3).\n",
    "\n",
    "Note that we define the steps to train the model following the **teacher-forcing paradigm**, i.e., the decoder is fed with the correct token at each step regardless of the model predictions at the previous steps. Training the network with the decoder prediction at the previous step would strengthen the model but it takes more time. The teacher-forcing approach is the one usually adopted to train seq2seq networks. \n",
    "\n",
    "Here below we use `tf.function` [documentation here](https://www.tensorflow.org/api_docs/python/tf/function) to increase the efficiency of the process, especially during training. You can find a complete explanation [here](https://www.tensorflow.org/guide/function). Note that you can also don't use this decorator and your code remains correct, this is just to give you an hint about more advanced features.\n",
    "\n",
    "To instantiate Encoder and Decoder entities, use ``input_text_processor.vocabulary_size()`` as the ``input_vocab_size``. See [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#vocabulary_size) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorModelTrain(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, input_text_processor, output_text_processor):\n",
    "        \"\"\"\n",
    "        Here we instantiate the encoder and decoder that you created as subclasses of the tf.keras.Layer class\n",
    "        :param embedding_dim: size of the embeddings\n",
    "        :param units: number of units in the encoding vector\n",
    "        :param input_text_processor: processing function for the input\n",
    "        :param output_text_processor: processing function for the output\n",
    "        \"\"\"\n",
    "        super(TranslatorModelTrain, self).__init__()\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # Instantiate an Encoder and a Decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Define the attributes of objects belonging to the TranslatorModelTrain class\n",
    "        # Use the input arguments and the entities you just created\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def preprocess(self, input_text, target_text):\n",
    "        \"\"\"\n",
    "        Helper function to preprocess the input\n",
    "        :param input_text: the input text to be translated\n",
    "        :param target_text: the correct output translation\n",
    "        :return: tokens and masks\n",
    "        \"\"\"\n",
    "        # Convert the text to token IDs\n",
    "        ### START CODE HERE ###\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        target_tokens = self.output_text_processor(target_text)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Convert IDs to masks\n",
    "        input_mask = input_tokens != 0\n",
    "        target_mask = target_tokens != 0\n",
    "        \n",
    "        return input_tokens, input_mask, target_tokens, target_mask\n",
    "\n",
    "    def loop_step(self, input_token, target_token, input_mask, enc_output, dec_state):\n",
    "        \"\"\"\n",
    "        Helper function used inside the train_step method\n",
    "        \"\"\"\n",
    "        # Run the decoder for one step (pass the proper arguments to self.decoder)\n",
    "        ### START CODE HERE ###\n",
    "        logits, _, dec_state = self.decoder(new_tokens=input_token, enc_output=enc_output,\n",
    "                                            mask=input_mask, state=dec_state)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute the loss using the self.loss\n",
    "        # Note that self.loss will be set when running the compile function \n",
    "        # see here: https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\n",
    "        y = target_token\n",
    "        y_pred = logits\n",
    "        ### START CODE HERE ###\n",
    "        step_loss = self.loss(y, y_pred)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return step_loss, dec_state\n",
    "    \n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                                   tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "    def train_step(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: the input of the Model (a batch)\n",
    "        :return: the loss on the current batch\n",
    "        \"\"\"\n",
    "        # Train with teacher forcing\n",
    "        input_text, target_text = inputs\n",
    "        \n",
    "        # Use the preprocess function you implemented above to obtain input_tokens, input_mask, target_tokens, target_mask\n",
    "        ### START CODE HERE ###\n",
    "        (input_tokens, input_mask, target_tokens, target_mask) = self.preprocess(input_text, target_text)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encode the input\n",
    "            ### START CODE HERE ###\n",
    "            enc_output, enc_state = self.encoder(input_tokens)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            # Initialize the decoder's state to the encoder's final state\n",
    "            # This only works if the encoder and decoder have the same number of units\n",
    "            # Initialize the loss to a constant equal to zero (use tf.constant(0.0))\n",
    "            ### START CODE HERE ###\n",
    "            dec_state = enc_state\n",
    "            loss = tf.constant(0.0)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            ### START CODE HERE ###\n",
    "            # Create a loop that iterate over the lenght of the target (max_target_length)\n",
    "            # Uset tf.range to create the array of indices used in the iteration\n",
    "            for t in tf.range(max_target_length - 1):\n",
    "            ### END CODE HERE ###\n",
    "                # Pass in two tokens from the target sequence:\n",
    "                # 1. The current input to the decoder [:, t:t+1] --> teacher-forcing training, i.e., you should take the input from target_tokens\n",
    "                # 2. The target for the decoder's next prediction [:, t+1:t+2]\n",
    "                input_token = target_tokens[:, t:t+1]\n",
    "                target_token = target_tokens[:, t+1:t+2]\n",
    "                \n",
    "                # Use the loop_step function implemented above to obtain step_loss and dec_state\n",
    "                # step_loss is added to the loss (cumulative over sequence)\n",
    "                # dec_state will be used in the next iteration (continuously updated with the new state)\n",
    "                ### START CODE HERE ###\n",
    "                step_loss, dec_state = self.loop_step(input_token, target_token, input_mask, enc_output, dec_state)\n",
    "                loss = loss + step_loss\n",
    "                ### END CODE HERE ###\n",
    "\n",
    "            # Average the loss over all non padding tokens.\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "        # Apply the optimization step: use self.trainable_variables to obtain the trainable weights, \n",
    "        # use tape.gradient to obtain the gradients of the loss with respect to the trainable weights, \n",
    "        # apply the optimizer using self.optimizer.apply_gradients\n",
    "        # Note that as for the self.loss, self.optimizer will be set when running the compile function \n",
    "        # see here: https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\n",
    "        ### START CODE HERE ###\n",
    "        variables = self.trainable_variables\n",
    "        gradients = tape.gradient(ave rage_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        # In our case we set the value of the batch_loss\n",
    "        return {'batch_loss': average_loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the training step works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:17:47.206523: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] function_optimizer failed: INVALID_ARGUMENT: Input 6 of node gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/PartitionedCall was passed variant from gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/TensorListPopBack_2:1 incompatible with expected float.\n",
      "2022-12-21 11:17:47.287233: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] layout failed: OUT_OF_RANGE: src_output = 25, but num_outputs is only 25\n",
      "2022-12-21 11:17:47.369826: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] tfg_optimizer{} failed: INVALID_ARGUMENT: Input 6 of node gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/PartitionedCall was passed variant from gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/TensorListPopBack_2:1 incompatible with expected float.\n",
      "\twhen importing GraphDef to MLIR module in GrapplerHook\n",
      "2022-12-21 11:17:47.387451: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] function_optimizer failed: INVALID_ARGUMENT: Input 6 of node gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/PartitionedCall was passed variant from gradient_tape/while/while_grad/body/_531/gradient_tape/while/gradients/while/decoder_1/gru_3/PartitionedCall_grad/TensorListPopBack_2:1 incompatible with expected float.\n",
      "2022-12-21 11:17:47.481170: W tensorflow/core/common_runtime/process_function_library_runtime.cc:866] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 1 of node while/body/_1/while/TensorListPushBack_56 was passed float from while/body/_1/while/decoder_1/gru_3/PartitionedCall:6 incompatible with expected variant.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.620607>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.557055>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.431663>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.0879264>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.0591316>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9354348>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.6499484>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.2087357>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.1088173>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.946425>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an object of the TranslatorModelTrain class\n",
    "# Note that we already defined the embedding_dim and units variables in section 1 of the notebook,\n",
    "# and the input_text_processor and output_text_processor in sections 1 and 3 respectively\n",
    "### START CODE HERE ###\n",
    "translator = TranslatorModelTrain(embedding_dim, units, input_text_processor=input_text_processor, output_text_processor=output_text_processor)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Configure the optimizer and the loss using the compile method\n",
    "### START CODE HERE ###\n",
    "translator.compile(optimizer=tf.optimizers.Adam(), loss=MaskedLoss())\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Call the train_step function for some steps\n",
    "for n in range(10):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Build and fit the translator model\n",
    "Let's first create a callback to be used during training to save the batch losses. To do that we sub-class the `tf.keras.callbacks.Callback class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        super(BatchLogs, self).__init__()\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instantiate the model for the translator and train it (use the `compile` and `fit` methods). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:27:51.406837: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] function_optimizer failed: INVALID_ARGUMENT: Input 6 of node StatefulPartitionedCall/gradient_tape/while/while_grad/body/_589/gradient_tape/while/gradients/while/decoder_3/gru_7/PartitionedCall_grad/PartitionedCall was passed variant from StatefulPartitionedCall/gradient_tape/while/while_grad/body/_589/gradient_tape/while/gradients/while/decoder_3/gru_7/PartitionedCall_grad/TensorListPopBack_2:1 incompatible with expected float.\n",
      "2022-12-21 11:27:51.493690: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] layout failed: OUT_OF_RANGE: src_output = 25, but num_outputs is only 25\n",
      "2022-12-21 11:27:51.576579: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] tfg_optimizer{} failed: INVALID_ARGUMENT: Input 6 of node StatefulPartitionedCall/gradient_tape/while/while_grad/body/_589/gradient_tape/while/gradients/while/decoder_3/gru_7/PartitionedCall_grad/PartitionedCall was passed variant from StatefulPartitionedCall/gradient_tape/while/while_grad/body/_589/gradient_tape/while/gradients/while/decoder_3/gru_7/PartitionedCall_grad/TensorListPopBack_2:1 incompatible with expected float.\n",
      "\twhen importing GraphDef to MLIR module in GrapplerHook\n",
      "2022-12-21 11:27:51.594619: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] function_optimizer failed: INVALID_ARGUMENT: Input 6 of node StatefulPartitionedCall/gradient_tape/while/while_grad/body/_589/gradient_tape/while/gradients/while/decoder_3/gru_7/PartitionedCall_grad/PartitionedCall was passed variant from StatefulPartitionedCall/gradient_tape/while/while_grad/body/_589/gradient_tape/while/gradients/while/decoder_3/gru_7/PartitionedCall_grad/TensorListPopBack_2:1 incompatible with expected float.\n",
      "2022-12-21 11:27:51.692050: W tensorflow/core/common_runtime/process_function_library_runtime.cc:866] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 1 of node StatefulPartitionedCall/while/body/_59/while/TensorListPushBack_56 was passed float from StatefulPartitionedCall/while/body/_59/while/decoder_3/gru_7/PartitionedCall:6 incompatible with expected variant.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 29s 69ms/step - batch_loss: 1.9798\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 26s 69ms/step - batch_loss: 0.8566\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 26s 69ms/step - batch_loss: 0.4327\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 26s 69ms/step - batch_loss: 0.2805\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 26s 68ms/step - batch_loss: 0.2149\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 26s 69ms/step - batch_loss: 0.1804\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 26s 69ms/step - batch_loss: 0.1637\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 26s 69ms/step - batch_loss: 0.1493\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 26s 69ms/step - batch_loss: 0.1406\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 27s 71ms/step - batch_loss: 0.1366\n"
     ]
    }
   ],
   "source": [
    "# Build the encoder and decoder as done before\n",
    "### START CODE HERE ###\n",
    "train_translator = TranslatorModelTrain(embedding_dim, units, input_text_processor=input_text_processor,\n",
    "                                        output_text_processor=output_text_processor)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Configure the optimizer and the loss using the compile method as done before\n",
    "### START CODE HERE ###\n",
    "train_translator.compile(optimizer=tf.optimizers.Adam(), loss=MaskedLoss())\n",
    "### END CODE HERE ###\n",
    "# Create an istance of the BatchLogs class to be used as a callback in the fit method below\n",
    "batch_loss = BatchLogs('batch_loss')\n",
    "\n",
    "# Fit the model and save the weights\n",
    "### START CODE HERE ###\n",
    "train_translator.fit(dataset_train, epochs=10, callbacks=[batch_loss])\n",
    "train_translator.save_weights('my_attention_model_weights')\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load presaved weights uncomment the line below\n",
    "# train_translator.load_weights('attention_model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CE/token')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 3])\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Build the translator that uses the trained model to translate an input sentence\n",
    "Now that the model is trained, here we implement the function thatexecute the full text to text translation.\n",
    "\n",
    "Overall this is similar to the training loop, except that the input to the decoder at each time step is the last decoder's prediction (no teacher-forcing during the testing phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorModelInference(tf.Module):\n",
    "    def __init__(self, encoder, decoder, input_text_processor, output_text_processor):\n",
    "        \"\"\"\n",
    "        Here we create a new class by subclassing the tf.Module one. tf.Module id the general class from which all the\n",
    "        other classes, e.g., tf.keras.Layer, tf.keras.Model, inherits.\n",
    "        See https://www.tensorflow.org/api_docs/python/tf/Module\n",
    "        :param encoder: the trained encoder\n",
    "        :param decoder: the trained decoder\n",
    "        :param input_text_processor: processing function for the input\n",
    "        :param output_text_processor: processing function for the output\n",
    "        \"\"\"\n",
    "        super(TranslatorModelInference, self).__init__()\n",
    "        \n",
    "        # Define the attributes of objects belonging to the TranslatorModelInference class\n",
    "        # Use the input arguments\n",
    "        ### START CODE HERE ###\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Instantiate a StringLookup layers to map integer indices into text using the given vocabulary\n",
    "        # see https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup\n",
    "        # use vocabulary=output_text_processor.get_vocabulary(), mask_token='', and invert=True\n",
    "        ### START CODE HERE ###\n",
    "        self.output_token_string_from_index = tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(), mask_token='', invert=True)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Instantiate a StringLookup layers to map text into integer indices using the given vocabulary\n",
    "        # see https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup\n",
    "        # use vocabulary=output_text_processor.get_vocabulary() and mask_token=''\n",
    "        ### START CODE HERE ###\n",
    "        index_from_string = tf.keras.layers.StringLookup(vocabulary=output_text_processor.get_vocabulary(),\n",
    "                                                         mask_token='')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # The output of the translator network should never generate padding, unknown, or start keys --> mask them\n",
    "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "        self.start_token = index_from_string(tf.constant('[START]'))\n",
    "        self.end_token = index_from_string(tf.constant('[END]'))\n",
    "\n",
    "    def tokens_to_text(self, result_tokens):\n",
    "        # Use self.output_token_string_from_index to convert the result_tokens indices into a strings\n",
    "        ### START CODE HERE ###\n",
    "        result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "        ### END CODE HERE ###\n",
    "        result_text = tf.strings.reduce_join(result_text_tokens, axis=1, separator=' ')\n",
    "        result_text = tf.strings.strip(result_text)\n",
    "        return result_text\n",
    "\n",
    "    def sample(self, logits):\n",
    "        # Set the logits for all masked tokens to -inf, so they are never chosen\n",
    "        logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "        logits = tf.squeeze(logits, axis=1)\n",
    "        # Use tf.random.categorical to obtain the new_tokens based on the logits\n",
    "        # see https://www.tensorflow.org/api_docs/python/tf/random/categorical\n",
    "        ### START CODE HERE ###\n",
    "        new_tokens = tf.random.categorical(logits, num_samples=1)\n",
    "        ### END CODE HERE ###\n",
    "        return new_tokens\n",
    "\n",
    "    def translate(self, input_text, max_length=50):\n",
    "        batch_size = tf.shape(input_text)[0]\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "        dec_state = enc_state\n",
    "        new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "        result_tokens = []\n",
    "        attention = []\n",
    "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Obtain the logits, attention_weights, and dec_state forwarding the new_tokens through self.decoder\n",
    "            # use enc_output=enc_output, mask=(input_tokens != 0), state=dec_state\n",
    "            ### START CODE HERE ###\n",
    "            logits, attention_weights, dec_state = self.decoder(new_tokens=new_tokens, enc_output=enc_output,\n",
    "                                                                mask=(input_tokens != 0), state=dec_state)\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            attention.append(attention_weights)\n",
    "\n",
    "            # Obtain the next token by using the self.sample function \n",
    "            ### START CODE HERE ###\n",
    "            new_tokens = self.sample(logits)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            # If a sequence produces an `end_token`, set it `done`\n",
    "            done = done | (new_tokens == self.end_token)\n",
    "            # Once a sequence is done it only produces 0-padding\n",
    "            new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "            # Collect the generated tokens\n",
    "            result_tokens.append(new_tokens)\n",
    "\n",
    "            if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "                break\n",
    "\n",
    "        # Convert the list of generates token indices to a list of strings\n",
    "        result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "        result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "        attention_stack = tf.concat(attention, axis=1)\n",
    "        return result_text, attention_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate here below a translator by using the trained encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the TranslatorModelInference\n",
    "### START CODE HERE ###\n",
    "translator = TranslatorModelInference(encoder=train_translator.encoder,\n",
    "                                      decoder=train_translator.decoder,\n",
    "                                      input_text_processor=input_text_processor,\n",
    "                                      output_text_processor=output_text_processor)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the translator to translate sentences! Let's first (first cell here below) try to use the `token_to_text` and the `sample` methods separately and then (next cell) use the complete `translate` method to obtain the English version of an Italian sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[  92],\n",
       "       [2588],\n",
       "       [  97],\n",
       "       [1724],\n",
       "       [2197]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_output_tokens = tf.random.uniform(shape=[5, 2], minval=0, dtype=tf.int64, maxval=output_text_processor.vocabulary_size())\n",
    "translator.tokens_to_text(example_output_tokens).numpy()\n",
    "\n",
    "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
    "example_output_tokens = translator.sample(example_logits)\n",
    "example_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom looks cool .\n",
      "this is my life .\n",
      "he paints .\n",
      "\n",
      "[0.9999998  1.         1.         0.99999994 1.0000001  1.0000001 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/francesca/5b8d09ed-c8f7-433b-8010-0753a08b1c7c/HDA_lab/labs2022-2023/Lab_7/dataset_utils.py:47: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "/media/francesca/5b8d09ed-c8f7-433b-8010-0753a08b1c7c/HDA_lab/labs2022-2023/Lab_7/dataset_utils.py:48: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_text = tf.constant(['Tom sembra rilassato.',\n",
    "                          'questa è la mia vita',\n",
    "                          'Lui vorrebbe cucinare.'])\n",
    "\n",
    "# Translate the input_text using the translate method of translator\n",
    "### START CODE HERE ###\n",
    "result, attention = translator.translate(input_text=input_text)\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()\n",
    "\n",
    "a = attention[1]\n",
    "\n",
    "print(np.sum(a, axis=-1))\n",
    "plt.figure()\n",
    "_ = plt.bar(range(len(a[0, :])), a[0, :])\n",
    "plt.figure()\n",
    "plt.imshow(np.array(a), vmin=0.0)\n",
    "\n",
    "i = 1\n",
    "plot_attention(attention[i], input_text[i], result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just created a neural machine translation model with attention, congratulations! If it is good enough you can also use it instead of Google translate :) If you want to increase the performance you can try to implement a [Transformer model](https://www.tensorflow.org/text/tutorials/transformer).\n",
    "\n",
    "**This was also the last HDA lab...we hope you enjoyed the laboratory sessions and you understood better the different learning strategies by implementing them in practice to solve human-related problems. Thank you for attending the labs and the best of luck for your future! :)**\n",
    "\n",
    "**Francesca and Silvia**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
