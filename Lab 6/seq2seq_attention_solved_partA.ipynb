{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFSnvZruYXt6"
      },
      "source": [
        "# Neural machine translation through sequence-to-sequence model with attention - Part A\n",
        "\n",
        "Welcome to the sixth HDA laboratory! In this notebook, you will start to implement a sequence-to-sequence network for sentence translation from Italian to English. Tomorrow, during the last HDA laboratory, you will conclude the implementation and test your translator!\n",
        "\n",
        "This notebook is a revisitation of the one proposed by [TensorFlow](https://www.tensorflow.org/text/tutorials/nmt_with_attention).\n",
        "\n",
        "**In this assignment, you will:**\n",
        "- Implement the basic building blocks of an encoder and a decoder for a seq2seq model. \n",
        "- Implement an attention mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acTVJMT2YXt_"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from dataset_utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks0HzpuRYXuB"
      },
      "source": [
        "## Load the dataset\n",
        "Download the dataset from [here](http://www.manythings.org/anki/) and unzip it in the same folder of the notebook. In this notebook the ita-eng one is chosen but you can select the one you prefer and change the first line below.\n",
        "The `buff_size` variable is to select a small portion of the dataset for reducing the complexity of the training, you can increase it to obtain a better translator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEkEy-z6YXuB"
      },
      "outputs": [],
      "source": [
        "path_to_file = pathlib.Path('./ita-eng/ita.txt')\n",
        "buff_size = 30000\n",
        "inp, targ = load_data(path_to_file, buff_size)\n",
        "print(inp[-2])\n",
        "print(targ[-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrHKnnhKYXuC"
      },
      "source": [
        "Split the data into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdwnm39qYXuC"
      },
      "outputs": [],
      "source": [
        "train_len = int(buff_size * 0.8)\n",
        "\n",
        "inp_train = inp[:train_len]\n",
        "targ_train = targ[:train_len]\n",
        "\n",
        "inp_test = inp[train_len:]\n",
        "targ_test = targ[train_len:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp3379wUYXuD"
      },
      "source": [
        "Create the training and test datasets. Use the function [tf.data.Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) to create the dataset with ``input`` and ``target`` pairs. Hence, cache the dataset, and apply the shuffling and the baching operations (see Lab 5 for reference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP5ez3hZYXuD"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "### START CODE HERE ###\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((inp_train, targ_train))\n",
        "cache_file_train = 'dataset_cache_train'\n",
        "dataset_train = dataset_train.cache(cache_file_train)\n",
        "dataset_train = dataset_train.shuffle(buff_size)\n",
        "dataset_train = dataset_train.batch(batch_size)\n",
        "### END CODE HERE ###\n",
        "\n",
        "### START CODE HERE ###\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((inp_test, targ_test))\n",
        "cache_file_test = 'dataset_cache_test'\n",
        "dataset_test = dataset_test.cache(cache_file_test)\n",
        "dataset_test = dataset_test.shuffle(buff_size)\n",
        "dataset_test = dataset_test.batch(batch_size)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESHmTlSsYXuE"
      },
      "source": [
        "Create a [tf.keras.layers.TextVectorization layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) that you will use in the following to map the text into integer sequences. We use the custom `tf_lower_and_split_punct` in `dataset_utils.py` for the standardization (e.g., remove accents, and put all letters in lowercase...). This operation is needed because we want to use a vocabulary with a limited size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN6VbPdDYXuF"
      },
      "outputs": [],
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
        "                                                         max_tokens=max_vocab_size)\n",
        "input_text_processor.adapt(inp_train)\n",
        "\n",
        "output_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
        "                                                          max_tokens=max_vocab_size)\n",
        "output_text_processor.adapt(targ_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCZfwU2nYXuG"
      },
      "source": [
        "Run the cell below to have an example of how the preprocessing is working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqSVymm8YXuG"
      },
      "outputs": [],
      "source": [
        "example_text = tf.constant('Hi! You\\'ve almost done, this is the sixth HDA lab. Oggi Ã¨ il sesto laboratorio del corso HDA!')\n",
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgnqPA2WYXuG"
      },
      "source": [
        "## 1 - Encoder\n",
        "Let's start to create the seq2seq model. \n",
        "<center><img src=\"images/seq2seq.jpg\" style=\"width:50%\"></center>\n",
        "<caption><center> The seq2seq model with attention.<br> </center></caption>\n",
        "\n",
        "The first block we need to code is the encoder. This block processes the input sequence and encodes it into a code of fixed size.\n",
        "\n",
        "To create the encoder we define a [custom tf.keras.Layer](https://www.tensorflow.org/guide/keras/custom_layers_and_models) by subclassing the tf.keras.Layer. We will instantiate all the sub-layers that we need for this new layer and combine them to create the new layer. \n",
        "Here in the ``init`` method we instantiate all the sub-layers needed to define the encoder. Each layer will be an attribut of the class, i.e., it will be referred to as ``self.attribute_name`` within the class.  \n",
        "Next, the `call` method creates the layer by combining the sub-layers instantiated above: first the input is forwarded through the embedding sub.layer and the output of the embedding is forwarded through the RNN with GRU sub-layer. \n",
        "    \n",
        "Specifically, through the ``call`` method, the encoder:\n",
        "1. takes a list of token indices\n",
        "2. converts each token into an embedding vector using a [layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
        "3. processes the embeddings sequentially through a [tf.keras.layers.GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) and obtain the output state (`return_state`=True) and the entire sequence of hidden states resulting from the processing of the entire sequence (`return_sequences`=True), use `glorot_uniform` for the initializer\n",
        "4. returns the processed sequence that will be passed to the attention layer and the internal state stat will serve to initialize the decoder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFdKHh9gYXuH"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "        \"\"\"\n",
        "        This class creates a new custom tf.keras.Layer. This is done by instantiating some attributes and combining them.\n",
        "        For more info see: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
        "        Here we instantiate all the layers that we need for this model. \n",
        "        Next, the `call` method create the layer by combining the ones instantiated here.\n",
        "        :param input_vocab_size: the size of the vocabulary\n",
        "        :param embedding_dim: fixed size for the embeddings\n",
        "        :param enc_units: number of neurons for the encoding\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.enc_units = enc_units\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "\n",
        "        # Instantiate an embedding layer to convert tokens into vectors\n",
        "        # see https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "        ### START CODE HERE ###\n",
        "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                                   embedding_dim)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Instantiate a GRU RNN layer to process those vectors sequentially\n",
        "        ### START CODE HERE ###\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def call(self, tokens, state=None):\n",
        "        \"\"\"\n",
        "        This function create the custom tf.keras.Layer structure by combining the blocks defined in the __init__\n",
        "        :param tokens: the input tokens\n",
        "        :param state: the info state for the GRU RNN (if present)\n",
        "        :return: the outout and the state of the GRU RNN\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "        vectors = self.embedding(tokens)\n",
        "        output, state = self.gru(vectors, initial_state=state)\n",
        "        ### END CODE HERE ###\n",
        "        return output, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6HXlf5eYXuH"
      },
      "source": [
        "Check the correct functioning of the encoder by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS-At6dWYXuI"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "for example_input_batch, example_target_batch in dataset_train.take(1):\n",
        "    print(example_input_batch[:5])\n",
        "    print()\n",
        "    print(example_target_batch[:5])\n",
        "    break\n",
        "\n",
        "# Convert the input text to tokens using the input_text_processor layer \n",
        "# that you created above as an instance of tf.keras.layers.TextVectorization\n",
        "### START CODE HERE ###\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Instantiate an object of the Encoder class\n",
        "### START CODE HERE ###\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Encode the input sequence using the Encoder object you have just instantiated\n",
        "### START CODE HERE ###\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIyYX6mVYXuI"
      },
      "source": [
        "## 2 - Attention layer\n",
        "The decoder uses attention to selectively focus on parts of the input sequence. The attention takes a sequence of vectors as input for each example and returns an attention vector for each example.\n",
        "We use the attention layer [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention) that follows the [Bahdanau-style attention](https://arxiv.org/pdf/1409.0473.pdf). \n",
        "The query is the decoder state ($\\boldsymbol{h}_t$) attending to the sequence while the value is the encoder output ($\\bar{\\boldsymbol{h}}_s$) being attended to.\n",
        "The attention weights and the context vector are computed as follows.\n",
        "\n",
        "$$\\large{\\rm{score}(\\boldsymbol{h}_t, \\boldsymbol{\\bar{h}}_s) = \\rm{tanh}(\\boldsymbol{W_1}\\boldsymbol{h}_t + \\boldsymbol{W_2}\\boldsymbol{\\bar{h}}_s)}$$\n",
        "\n",
        "<center><img src=\"images/attention_equation_1.jpeg\" style=\"width:50%\"></center>\n",
        "<center><img src=\"images/attention_equation_2.jpeg\" style=\"width:50%\"></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2NreM-7YXuI"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        \"\"\"\n",
        "        This class creates a new custom tf.keras.Layer. This is done by instantiating some attributes and combining them.\n",
        "        For more info see: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
        "        Here we instantiate all the layers that we need for this model. \n",
        "        Next, the `call` method create the layer by combining the ones instantiated here.\n",
        "        :param units: number of units for the Dense layer\n",
        "        \"\"\"\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        \n",
        "        # Instantiate two Dense layers for the query (decoder state) and for the value (encoder output)\n",
        "        # with 'units' output neurons, use use_bias=False\n",
        "        ### START CODE HERE (2 lines) ### \n",
        "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Instantiate a AdditiveAttention layer\n",
        "        ### START CODE HERE ###\n",
        "        self.attention = tf.keras.layers.AdditiveAttention()\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def call(self, query, value, mask):\n",
        "\n",
        "        # Pass the query (the decoder state attending to the sequence) through the first dense layer\n",
        "        ### START CODE HERE ###\n",
        "        w1_query = self.W1(query)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Pass the value (the sequence of encoder outputs being attended to) through the second dense layer\n",
        "        ### START CODE HERE ###\n",
        "        w2_key = self.W2(value)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "        value_mask = mask\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        context_vector, attention_weights = self.attention(inputs=[w1_query, value, w2_key],\n",
        "                                                           mask=[query_mask, value_mask],\n",
        "                                                           return_attention_scores=True)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1S5VnppYXuJ"
      },
      "source": [
        "Check the correct functioning of the attention layer by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bh6eWZeYXuJ"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "attention_layer = BahdanauAttention(units)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens, the mask is used to exclude the padding\n",
        "# query=example_attention_query\n",
        "# value=example_enc_output\n",
        "# mask=(example_tokens != 0)\n",
        "### START CODE HERE ###\n",
        "context_vector, attention_weights = attention_layer(query=example_attention_query,\n",
        "                                                    value=example_enc_output,\n",
        "                                                    mask=(example_tokens != 0))\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units): {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxnaqeeTYXuJ"
      },
      "outputs": [],
      "source": [
        "attention_slice = attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]\n",
        "\n",
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "# zoom in\n",
        "top = max(plt.ylim())\n",
        "zoom = 0.85*top\n",
        "plt.ylim([0.90*top, top])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5nq4NTqYXuK"
      },
      "source": [
        "## 3 - Decoder\n",
        "The decoder generates the prediction for the next output token starting from the encoder output\n",
        "\n",
        "1. it uses an RNN to keep track of what it has predicted previously\n",
        "2. the RNN output is used as a query for the attention layer to attend the encoder's output and produce the context vector\n",
        "3. the context vector is combined with the RNN output to generate the attention vector using the equation\n",
        "\n",
        "$$\\large{\\boldsymbol{a}_t = \\rm{tanh}(\\boldsymbol{W}_c[\\boldsymbol{c}_t; \\boldsymbol{h}_t])}~~~~~\\rm{[attention~vector]}$$\n",
        "\n",
        "4. finally, it generates logit predictions for the next token based on the attention vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4nc7q4EYXuK"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "        \"\"\"\n",
        "        This class creates a new custom tf.keras.Layer. This is done by instantiating some variables and combining them.\n",
        "        For more info see: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
        "        Here we instantiate all the layers that we need for this model. \n",
        "        Next, the `call` method create the layer by combining the ones instantiated here.\n",
        "        :param output_vocab_size:\n",
        "        :param embedding_dim:\n",
        "        :param dec_units:\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.dec_units = dec_units\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Instantiate an embedding layer to convet token IDs to vectors\n",
        "        ### START CODE HERE ###\n",
        "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size, embedding_dim)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Instantiate an RNN to keep track of what's wii be generated from time to time by the decoder\n",
        "        ### START CODE HERE ###\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Instantiate a BahdanauAttention layer that will obtain as input the RNN output, i.e., the RNN output\n",
        "        # will be the query to the attention layer over the encoder's output to produce the context vector\n",
        "        ### START CODE HERE ###\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Instantiate a Dense layer to combine the RNN output and the context vector to generate the attention vector\n",
        "        ### START CODE HERE ###\n",
        "        self.Wc = tf.keras.layers.Dense(self.dec_units, activation=tf.math.tanh, use_bias=False)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Instantiate a fully connected layer to produce the logits for each output token based on the attention vector\n",
        "        ### START CODE HERE ###\n",
        "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def call(self, new_tokens, enc_output, mask, state=None):\n",
        "        # Step 1. Lookup the embeddings\n",
        "        ### START CODE HERE ###\n",
        "        vectors = self.embedding(new_tokens)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Step 2. Process one step with the RNN\n",
        "        ### START CODE HERE ###\n",
        "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Step 3. Use the RNN output as the query for the attention over the encoder output\n",
        "        ### START CODE HERE ###\n",
        "        context_vector, attention_weights = self.attention(query=rnn_output, value=enc_output, mask=mask)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Step 4. Concatenate the context_vector and rnn_output -- shape: (batch t, value_units + query_units)\n",
        "        ### START CODE HERE ###\n",
        "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Step 4. Obtain the attention vector: attention_vector = tanh(Wc@context_and_rnn_output)\n",
        "        ### START CODE HERE ###\n",
        "        attention_vector = self.Wc(context_and_rnn_output)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Step 5. Generate logit predictions through the final dense layer\n",
        "        ### START CODE HERE ###\n",
        "        logits = self.fc(attention_vector)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return logits, attention_weights, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUPV0RhJYXuL"
      },
      "source": [
        "Check the correct functioning of the decoder layer by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2moZM2NYXuM"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\n",
        "### END CODE HERE ###\n",
        "    \n",
        "# Convert the target sequence using the output_text_processor object you instantiated above\n",
        "### START CODE HERE ###\n",
        "example_output_tokens = output_text_processor(example_target_batch)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Collect the \"[START]\" tokens\n",
        "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n",
        "\n",
        "# Run the decoder\n",
        "### START CODE HERE ###\n",
        "dec_logits, dec_attention, dec_state = decoder(first_token, enc_output=example_enc_output, mask=(example_tokens != 0), state = example_enc_state)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')\n",
        "\n",
        "sampled_token = tf.random.categorical(dec_logits[:, 0, :], num_samples=1)\n",
        "vocab = np.array(output_text_processor.get_vocabulary())\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWu0M4hVYXuM"
      },
      "source": [
        "## 4 - Loss function\n",
        "Here we create a custom loss function that will be used during training for computing the gradients to be backpropagated and allow updating the weights. The construction is similar to the one seen for the custom layers above,\n",
        "[see here for more info](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioCiQp0CYXuM"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Here we create a custom loss function. The construction is similar to the one seen for the custom layers above.\n",
        "        See https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses\n",
        "        \"\"\"\n",
        "        super(MaskedLoss, self).__init__()\n",
        "        \n",
        "        self.name = 'masked_loss'\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # Use the correct boolean for the from_logits argument\n",
        "        # Use reduction='none' as you will compute the reduced sum in the __call__ method\n",
        "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        # Calculate the loss for each item in the batch\n",
        "        ### START CODE HERE ###\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Mask off the losses on padding\n",
        "        mask = tf.cast(y_true != 0, tf.float32)\n",
        "        loss *= mask\n",
        "\n",
        "        # Return the total (use tf.reduce_sum)\n",
        "        ### START CODE HERE ###\n",
        "        return tf.reduce_sum(loss)\n",
        "        ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4efCkhIYXuN"
      },
      "source": [
        "Congratulations! You created all the building blocks to implement the sequence-to-sequence neural machine translator. Tomorrow you will create the network, train it and see it at work...ready? See you tomorrow :)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}